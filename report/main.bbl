% Generated by acm-fa.bst,  version: 0.6 (2011/07/01), for XePersian Package
% Authors: M.Amintoosi and M.Vahedi
\providecommand{\noopsort}[1]{}
\begin{thebibliography}{10}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{antol2015vqa}
\textsc{ Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,
  Lawrence~Zitnick, C., and Parikh, D.}
\newblock Vqa: Visual question answering.
\newblock  in {\em Proceedings of the IEEE international conference on computer
  vision\/} (2015),  pp. 2425--2433.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{ImageCLEFVQA-Med2019}
\textsc{ {Ben Abacha}, A., Hasan, S.~A., Datla, V.~V., Liu, J., Demner-Fushman,
  D., and M\"uller, H.}
\newblock {VQA-Med}: Overview of the medical visual question answering task at
  imageclef 2019.
\newblock  in {\em CLEF2019 Working Notes\/} (Lugano, Switzerland, September
  09-12 2019), {CEUR} Workshop Proceedings, CEUR-WS.org
  $<$http://ceur-ws.org$>$.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{chen2020uniter}
\textsc{ Chen, Y.-C., Li, L., Yu, L., Kholy, A.~E., Ahmed, F., Gan, Z., Cheng,
  Y., and Liu, J.}
\newblock Uniter: Universal image-text representation learning, 2020.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{devlin-etal-2019-bert}
\textsc{ Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.}
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock  in {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)\/} (Minneapolis, Minnesota,
  June 2019), Association for Computational Linguistics,  pp. 4171--4186.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{frankle2019lottery}
\textsc{ Frankle, J., and Carbin, M.}
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks, 2019.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{gan2021playing}
\textsc{ Gan, Z., Chen, Y.-C., Li, L., Chen, T., Cheng, Y., Wang, S., and Liu,
  J.}
\newblock Playing lottery tickets with vision and language, 2021.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{balanced_vqa_v2}
\textsc{ Goyal, Y., Khot, T., Summers{-}Stay, D., Batra, D., and Parikh, D.}
\newblock Making the {V} in {VQA} matter: Elevating the role of image
  understanding in {V}isual {Q}uestion {A}nswering.
\newblock  in {\em Conference on Computer Vision and Pattern Recognition
  (CVPR)\/} (2017).

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{gurari2018vizwiz}
\textsc{ Gurari, D., Li, Q., Stangl, A.~J., Guo, A., Lin, C., Grauman, K., Luo,
  J., and Bigham, J.~P.}
\newblock Vizwiz grand challenge: Answering visual questions from blind people,
  2018.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{li2020oscar}
\textsc{ Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu,
  H., Dong, L., Wei, F., Choi, Y., and Gao, J.}
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock {\em ECCV 2020\/} (2020).

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{lin2015microsoft}
\textsc{ Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays,
  J., Perona, P., Ramanan, D., Zitnick, C.~L., and Doll√°r, P.}
\newblock Microsoft coco: Common objects in context, 2015.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{ren15fasterrcnn}
\textsc{ Shaoqing~Ren, Kaiming~He, R. G. J.~S.}
\newblock {Faster R-CNN}: Towards real-time object detection with region
  proposal networks.
\newblock {\em arXiv preprint arXiv:1506.01497\/} (2015).

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{tan2019lxmert}
\textsc{ Tan, H.~H., and Bansal, M.}
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock  in {\em EMNLP/IJCNLP\/} (2019).

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{wang2020minivlm}
\textsc{ Wang, J., Hu, X., Zhang, P., Li, X., Wang, L., Zhang, L., Gao, J., and
  Liu, Z.}
\newblock Minivlm: A smaller and faster vision-language model, 2020.

\end{LTRbibitems}

\end{thebibliography}
